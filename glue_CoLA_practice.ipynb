{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xutian1113/pytorch_practice/blob/main/glue_CoLA_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3DKGQSuzHkO",
    "outputId": "8c693462-1dab-4f37-e9f6-60f7fe37747a"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bZ51UHG8X8p"
   },
   "source": [
    "## CoLA Description\n",
    "CoLA (Corpus of Linguistic Acceptability) is a dataset used for evaluating a model's ability to determine whether a given sentence is gramatically acceptable or unacceptable.\n",
    "- Task Type: binary classification (Acceptable or Unacceptable)\n",
    "- Goal: Predict whether a given sentence is grammatically acceptable (1) or unacceptable (0).\n",
    "- Data Source: Sentences are taken from published linguistic literature.\n",
    "- Label Distribution:\n",
    "  - 1 (Acceptable Sentence) → The sentence follows standard English grammar.\n",
    "  - 0 (Unacceptable Sentence) → The sentence is grammatically incorrect.\n",
    "  \n",
    "### CoLA Dataset Statistics\n",
    "| **Split**      | **Number of Samples** |\n",
    "|---------------|----------------------|\n",
    "| **Train**     | 8,551                |\n",
    "| **Validation** | 1,043                |\n",
    "| **Test**      | ~1,000 (Labels not public) |\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['sentence', 'label', 'idx'],\n",
    "        num_rows: 8551\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['sentence', 'label', 'idx'],\n",
    "        num_rows: 1043\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['sentence', 'label', 'idx'],\n",
    "        num_rows: 1063\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "### dataset[\"train\"][0]\n",
    "```\n",
    "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
    " 'label': 1,\n",
    " 'idx': 0}\n",
    "```\n",
    "\n",
    "### dataset statistics\n",
    "- train: ({0: 2528, 1: 6023}, 0.704362062916618)\n",
    "- val: ({(0: 322, 1: 721}, 0.6912751677852349)\n",
    "- test: unavailable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3LfGQE1OzakD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kldc6gLczYaW"
   },
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "  correct = (predicted == labels).sum().item()\n",
    "  return correct / len(labels)\n",
    "\n",
    "def f1_score(preds, labels):\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "  tp = ((predicted == 1) & (labels == 1)).sum().item()\n",
    "  fp = ((predicted == 1) & (labels == 0)).sum().item()\n",
    "  fn = ((predicted == 0) & (labels == 1)).sum().item()\n",
    "  precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "  recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "  return f1\n",
    "\n",
    "def precision(preds, labels):\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "  tp = ((predicted == 1) & (labels == 1)).sum().item()\n",
    "  fp = ((predicted == 1) & (labels == 0)).sum().item()\n",
    "  precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "  return precision\n",
    "\n",
    "\n",
    "def auc_roc(preds, labels):\n",
    "  # Convert predictions and labels to NumPy arrays\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "  preds_np = predicted.detach().cpu().numpy()\n",
    "  labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "  # Calculate AUC-ROC using scikit-learn's roc_auc_score\n",
    "  auc_score = roc_auc_score(labels_np, preds_np)\n",
    "\n",
    "  return auc_score\n",
    "\n",
    "\n",
    "def auc_pr(preds, labels):\n",
    "  \"\"\"\n",
    "  Calculates the AUPRC score for binary classification.\n",
    "\n",
    "  Args:\n",
    "    preds: Predicted probabilities (tensor) for the positive class.\n",
    "    labels: True labels (tensor) (0 or 1).\n",
    "\n",
    "  Returns:\n",
    "    AUPRC score (float).\n",
    "  \"\"\"\n",
    "  # Convert predictions and labels to NumPy arrays\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "  preds_np = predicted.detach().cpu().numpy()\n",
    "  labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "  # Calculate AUPRC using scikit-learn's average_precision_score\n",
    "  auprc_score = average_precision_score(labels_np, preds_np)\n",
    "\n",
    "  return auprc_score\n",
    "\n",
    "def confusion_matrix_func(preds, labels, num_classes=2):\n",
    "  \"\"\"\n",
    "  Calculates the confusion matrix.\n",
    "\n",
    "  Args:\n",
    "    preds: Predicted labels (tensor).\n",
    "    labels: True labels (tensor).\n",
    "    num_classes: Number of classes. Defaults to 2 for binary classification.\n",
    "\n",
    "  Returns:\n",
    "    Confusion matrix (NumPy array).\n",
    "  \"\"\"\n",
    "  # Get predicted class indices\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "\n",
    "  # Convert predictions and labels to NumPy arrays\n",
    "  predicted_np = predicted.detach().cpu().numpy()\n",
    "  labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "  # Calculate confusion matrix using scikit-learn's confusion_matrix\n",
    "  cm = confusion_matrix(labels_np, predicted_np, labels=range(num_classes))\n",
    "\n",
    "  return cm\n",
    "\n",
    "def recall(preds, labels):\n",
    "  \"\"\"\n",
    "  Calculates the recall score for binary classification.\n",
    "\n",
    "  Args:\n",
    "    preds: Predicted labels (tensor).\n",
    "    labels: True labels (tensor).\n",
    "\n",
    "  Returns:\n",
    "    Recall score (float).\n",
    "  \"\"\"\n",
    "  # Get predicted class indices\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "\n",
    "  # Calculate true positives (TP), false negatives (FN)\n",
    "  tp = ((predicted == 1) & (labels == 1)).sum().item()\n",
    "  fn = ((predicted == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "  # Calculate recall\n",
    "  recall_score = tp / (tp + fn) if tp + fn > 0 else 0  # Avoid division by zero\n",
    "\n",
    "  return recall_score\n",
    "\n",
    "def MCC(preds, labels):\n",
    "  \"\"\"\n",
    "  Calculates the Matthews correlation coefficient (MCC) for binary classification.\n",
    "\n",
    "  Args:\n",
    "    preds: Predicted labels (tensor).\n",
    "    labels: True labels (tensor).\n",
    "\n",
    "  Returns:\n",
    "    MCC score (float).\n",
    "    \"\"\"\n",
    "  # Get predicted class indices\n",
    "  _, predicted = torch.max(preds, 1)\n",
    "\n",
    "  # Convert predictions and labels to NumPy arrays\n",
    "  predicted_np = predicted.detach().cpu().numpy()\n",
    "  labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "  # Calculate MCC using scikit-learn's matthews_corrcoef\n",
    "  mcc_score = matthews_corrcoef(predicted_np, labels_np)\n",
    "  return mcc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htlEFcFcFfPL",
    "outputId": "5aebfb55-5ed4-489a-8ca5-5d14eea54998"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. load the GLUE MRPC dataset\n",
    "dataset = load_dataset('glue', 'cola')\n",
    "\n",
    "# 2. load the tokenizer for bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# converting raw text into numerical representations that can be processed by BERT model.\n",
    "\n",
    "# 3. define a tokenization function for sentence pair\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "'''\n",
    "input_ids: Tokenized numerical representation of both sentences.\n",
    "token_type_ids: Differentiates sentence1 (0) from sentence2 (1).\n",
    "attention_mask: Indicates which tokens are real (1) and padding (0).\n",
    "'''\n",
    "# 4. Tokenize the dataset (batched processing)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 5. Remove unnecessary columns and set the format to PyTorch tensors\n",
    "# Remove columns that are not used by the model (like the original sentences and index)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# 6. Create DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8)\n",
    "\n",
    "\n",
    "# 7. Set up the device, model, optimizer, and scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 20\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5XlCzv9N1aP2",
    "outputId": "9e70db89-399a-4510-8070-093cca65c3b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1069/1069 [00:51<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.4989, Accuracy: 0.7659, AUC-ROC: 0.6699, Recall: 0.9047, MCC: 0.3908\n",
      "Validation - Loss: 0.4441,  Accuracy: 0.8006, AUC-ROC: 0.7028, Recall: 0.9584, MCC: 0.5026\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1069/1069 [00:51<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 0.2825, Accuracy: 0.8862, AUC-ROC: 0.8553, Recall: 0.9309, MCC: 0.7229\n",
      "Validation - Loss: 0.3951,  Accuracy: 0.8265, AUC-ROC: 0.7894, Recall: 0.8863, MCC: 0.5878\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1069/1069 [00:51<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 0.1593, Accuracy: 0.9415, AUC-ROC: 0.9267, Recall: 0.9630, MCC: 0.8588\n",
      "Validation - Loss: 0.4973,  Accuracy: 0.8236, AUC-ROC: 0.7701, Recall: 0.9098, MCC: 0.5712\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1069/1069 [00:51<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 0.0924, Accuracy: 0.9678, AUC-ROC: 0.9610, Recall: 0.9778, MCC: 0.9227\n",
      "Validation - Loss: 0.6177,  Accuracy: 0.8207, AUC-ROC: 0.7603, Recall: 0.9182, MCC: 0.5611\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1069/1069 [00:51<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 0.0740, Accuracy: 0.9757, AUC-ROC: 0.9706, Recall: 0.9831, MCC: 0.9416\n",
      "Validation - Loss: 0.6008,  Accuracy: 0.8025, AUC-ROC: 0.7514, Recall: 0.8849, MCC: 0.5231\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1069/1069 [00:51<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 0.0414, Accuracy: 0.9856, AUC-ROC: 0.9831, Recall: 0.9892, MCC: 0.9655\n",
      "Validation - Loss: 0.9687,  Accuracy: 0.8054, AUC-ROC: 0.7295, Recall: 0.9279, MCC: 0.5171\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1069/1069 [00:51<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 0.0420, Accuracy: 0.9863, AUC-ROC: 0.9836, Recall: 0.9902, MCC: 0.9672\n",
      "Validation - Loss: 0.9233,  Accuracy: 0.8111, AUC-ROC: 0.7491, Recall: 0.9112, MCC: 0.5369\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1069/1069 [00:51<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 0.0303, Accuracy: 0.9898, AUC-ROC: 0.9872, Recall: 0.9937, MCC: 0.9755\n",
      "Validation - Loss: 0.9443,  Accuracy: 0.8198, AUC-ROC: 0.7485, Recall: 0.9348, MCC: 0.5554\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1069/1069 [00:51<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 0.0289, Accuracy: 0.9891, AUC-ROC: 0.9869, Recall: 0.9924, MCC: 0.9739\n",
      "Validation - Loss: 0.9854,  Accuracy: 0.8063, AUC-ROC: 0.7379, Recall: 0.9168, MCC: 0.5222\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1069/1069 [00:51<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Loss: 0.0204, Accuracy: 0.9917, AUC-ROC: 0.9897, Recall: 0.9945, MCC: 0.9801\n",
      "Validation - Loss: 1.2007,  Accuracy: 0.8111, AUC-ROC: 0.7396, Recall: 0.9265, MCC: 0.5332\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11:  29%|██▊       | 306/1069 [00:14<00:36, 20.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b77f8bcdfa77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Move batch to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b77f8bcdfa77>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Move batch to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "patience = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        # Move batch to the device\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        batch['labels'] = batch['label']\n",
    "        del batch['label']\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        all_preds.append(outputs.logits.detach())\n",
    "        all_labels.append(batch['labels'].detach())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    train_acc = accuracy(all_preds, all_labels)\n",
    "    train_auc = auc_roc(all_preds, all_labels)\n",
    "    train_rec = recall(all_preds, all_labels)\n",
    "    train_avg_loss = total_loss / len(train_dataloader)\n",
    "    train_mcc = MCC(all_preds, all_labels)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_avg_loss:.4f}, Accuracy: {train_acc:.4f}, AUC-ROC: {train_auc:.4f}, Recall: {train_rec:.4f}, MCC: {train_mcc:.4f}\")\n",
    "\n",
    "    # 9. Validation loop\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        batch['labels'] = batch['label']\n",
    "        del batch['label']\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "        logits = outputs.logits\n",
    "        val_preds.append(logits.detach())\n",
    "        val_labels.append(batch['labels'].detach())\n",
    "\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_acc = accuracy(val_preds, val_labels)\n",
    "    val_auc = auc_roc(val_preds, val_labels)\n",
    "    val_rec = recall(val_preds, val_labels)\n",
    "    val_avg_loss = val_loss / len(val_dataloader)\n",
    "    val_mcc = MCC(val_preds, val_labels)\n",
    "    if val_avg_loss < best_val_loss:\n",
    "        best_val_loss = val_avg_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "      epochs_no_improve += 1\n",
    "      # if epochs_no_improve >= patience:\n",
    "      #     print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss for {patience} epochs.\")\n",
    "      #     break\n",
    "    print(f\"Validation - Loss: {val_avg_loss:.4f},  Accuracy: {val_acc:.4f}, AUC-ROC: {val_auc:.4f}, Recall: {val_rec:.4f}, MCC: {val_mcc:.4f}\")\n",
    "\n",
    "\n",
    "    model.train()  # Switch back to training mode for the next epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C9xBMbu1nWR"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=8)\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = {key: value.to(device) for key, value in batch.items()}\n",
    "    batch['labels'] = batch['label']\n",
    "    del batch['label']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    test_preds.append(logits.detach())\n",
    "    test_labels.append(batch['labels'].detach())\n",
    "\n",
    "test_preds = torch.cat(test_preds)\n",
    "test_labels = torch.cat(test_labels)\n",
    "\n",
    "test_acc = accuracy(test_preds, test_labels)\n",
    "test_auc = auc_roc(test_preds, test_labels)\n",
    "test_rec = recall(test_preds, test_labels)\n",
    "print(f\"Test - Accuracy: {test_acc:.4f}, AUC-ROC: {test_auc:.4f}, Recall: {test_rec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P50LjKK7dq2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOUmMtydpdhruDKn2Y/3sAP",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
