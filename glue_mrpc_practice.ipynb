{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNU7J2fwRJkZ7NohIDX1skq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xutian1113/pytorch_practice/blob/main/glue_mrpc_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3DKGQSuzHkO",
        "outputId": "138cad31-3139-4eb1-cad3-ffea7fc035a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n"
      ],
      "metadata": {
        "id": "3LfGQE1OzakD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "  correct = (predicted == labels).sum().item()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def f1_score(preds, labels):\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "  tp = ((predicted == 1) & (labels == 1)).sum().item()\n",
        "  fp = ((predicted == 1) & (labels == 0)).sum().item()\n",
        "  fn = ((predicted == 0) & (labels == 1)).sum().item()\n",
        "  precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "  recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "  f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "  return f1\n",
        "\n",
        "def precision(preds, labels):\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "  tp = ((predicted == 1) & (labels == 1)).sum().item()\n",
        "  fp = ((predicted == 1) & (labels == 0)).sum().item()\n",
        "  precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "  return precision\n",
        "\n",
        "\n",
        "def auc_roc(preds, labels):\n",
        "  # Convert predictions and labels to NumPy arrays\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "  preds_np = predicted.detach().cpu().numpy()\n",
        "  labels_np = labels.detach().cpu().numpy()\n",
        "\n",
        "  # Calculate AUC-ROC using scikit-learn's roc_auc_score\n",
        "  auc_score = roc_auc_score(labels_np, preds_np)\n",
        "\n",
        "  return auc_score\n",
        "\n",
        "\n",
        "def auc_pr(preds, labels):\n",
        "  \"\"\"\n",
        "  Calculates the AUPRC score for binary classification.\n",
        "\n",
        "  Args:\n",
        "    preds: Predicted probabilities (tensor) for the positive class.\n",
        "    labels: True labels (tensor) (0 or 1).\n",
        "\n",
        "  Returns:\n",
        "    AUPRC score (float).\n",
        "  \"\"\"\n",
        "  # Convert predictions and labels to NumPy arrays\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "  preds_np = predicted.detach().cpu().numpy()\n",
        "  labels_np = labels.detach().cpu().numpy()\n",
        "\n",
        "  # Calculate AUPRC using scikit-learn's average_precision_score\n",
        "  auprc_score = average_precision_score(labels_np, preds_np)\n",
        "\n",
        "  return auprc_score\n",
        "\n",
        "def confusion_matrix_func(preds, labels, num_classes=2):\n",
        "  \"\"\"\n",
        "  Calculates the confusion matrix.\n",
        "\n",
        "  Args:\n",
        "    preds: Predicted labels (tensor).\n",
        "    labels: True labels (tensor).\n",
        "    num_classes: Number of classes. Defaults to 2 for binary classification.\n",
        "\n",
        "  Returns:\n",
        "    Confusion matrix (NumPy array).\n",
        "  \"\"\"\n",
        "  # Get predicted class indices\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "\n",
        "  # Convert predictions and labels to NumPy arrays\n",
        "  predicted_np = predicted.detach().cpu().numpy()\n",
        "  labels_np = labels.detach().cpu().numpy()\n",
        "\n",
        "  # Calculate confusion matrix using scikit-learn's confusion_matrix\n",
        "  cm = confusion_matrix(labels_np, predicted_np, labels=range(num_classes))\n",
        "\n",
        "  return cm\n",
        "\n",
        "def recall(preds, labels):\n",
        "  \"\"\"\n",
        "  Calculates the recall score for binary classification.\n",
        "\n",
        "  Args:\n",
        "    preds: Predicted labels (tensor).\n",
        "    labels: True labels (tensor).\n",
        "\n",
        "  Returns:\n",
        "    Recall score (float).\n",
        "  \"\"\"\n",
        "  # Get predicted class indices\n",
        "  _, predicted = torch.max(preds, 1)\n",
        "\n",
        "  # Calculate true positives (TP), false negatives (FN)\n",
        "  tp = ((predicted == 1) & (labels == 1)).sum().item()\n",
        "  fn = ((predicted == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "  # Calculate recall\n",
        "  recall_score = tp / (tp + fn) if tp + fn > 0 else 0  # Avoid division by zero\n",
        "\n",
        "  return recall_score\n",
        "\n"
      ],
      "metadata": {
        "id": "kldc6gLczYaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. load the GLUE MRPC dataset\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "\n",
        "# 2. load the tokenizer for bert-base-cased\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# converting raw text into numerical representations that can be processed by BERT model.\n",
        "'''\n",
        "\n",
        "1. splits input text into tokens\n",
        "2. maps tokens to numerical IDs using a vocabulary\n",
        "3. adds special tokens required by BERT\n",
        "4. handles padding and truncation to ensure uniform input size\n",
        "\n",
        "'''\n",
        "\n",
        "# 3. define a tokenization function for sentence pair\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "'''\n",
        "input_ids: Tokenized numerical representation of both sentences.\n",
        "token_type_ids: Differentiates sentence1 (0) from sentence2 (1).\n",
        "attention_mask: Indicates which tokens are real (1) and padding (0).\n",
        "'''\n",
        "# 4. Tokenize the dataset (batched processing)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVNSKl0H09yG",
        "outputId": "899d1623-42a3-47fd-ab9b-77b93062aca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Remove unnecessary columns and set the format to PyTorch tensors\n",
        "# Remove columns that are not used by the model (like the original sentences and index)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "mbZzL30l1NVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create DataLoaders for training and validation\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8)\n",
        "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8)"
      ],
      "metadata": {
        "id": "TT7AC9QU1TCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set up the device, model, optimizer, and scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader) * epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1g4g86b1VN_",
        "outputId": "08afaff8-a520-491e-d843-7765e77a6efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ig1KUcVo1X7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
        "        # Move batch to the device\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "        batch['labels'] = batch['label']\n",
        "        del batch['label']\n",
        "        # Forward pass\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        all_preds.append(outputs.logits.detach())\n",
        "        all_labels.append(batch['labels'].detach())\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    train_acc = accuracy(all_preds, all_labels)\n",
        "    train_auc = auc_roc(all_preds, all_labels)\n",
        "    train_rec = recall(all_preds, all_labels)\n",
        "    train_avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_avg_loss:.4f}, Accuracy: {train_acc:.4f}, AUC-ROC: {train_auc:.4f}, Recall: {train_rec:.4f}\")\n",
        "\n",
        "    # 9. Validation loop\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    for batch in val_dataloader:\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "        batch['labels'] = batch['label']\n",
        "        del batch['label']\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        val_preds.append(logits.detach())\n",
        "        val_labels.append(batch['labels'].detach())\n",
        "\n",
        "    val_preds = torch.cat(val_preds)\n",
        "    val_labels = torch.cat(val_labels)\n",
        "    val_acc = accuracy(val_preds, val_labels)\n",
        "    val_auc = auc_roc(val_preds, val_labels)\n",
        "    val_rec = recall(val_preds, val_labels)\n",
        "    print(f\"Validation - Accuracy: {val_acc:.4f}, AUC-ROC: {val_auc:.4f}, Recall: {val_rec:.4f}\")\n",
        "\n",
        "\n",
        "    model.train()  # Switch back to training mode for the next epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XlCzv9N1aP2",
        "outputId": "786be9e1-7b73-4b2b-9cf5-fdeea36393e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 459/459 [00:22<00:00, 20.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.5328, Accuracy: 0.7385, AUC-ROC: 0.6612, Recall: 0.8828\n",
            "Validation - Accuracy: 0.8627, AUC-ROC: 0.8350, Recall: 0.9104\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 459/459 [00:21<00:00, 20.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Loss: 0.2703, Accuracy: 0.8969, AUC-ROC: 0.8796, Recall: 0.9293\n",
            "Validation - Accuracy: 0.8407, AUC-ROC: 0.8022, Recall: 0.9068\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 459/459 [00:21<00:00, 20.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - Loss: 0.0911, Accuracy: 0.9689, AUC-ROC: 0.9622, Recall: 0.9814\n",
            "Validation - Accuracy: 0.8554, AUC-ROC: 0.8213, Recall: 0.9140\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 459/459 [00:21<00:00, 20.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 - Loss: 0.0513, Accuracy: 0.9836, AUC-ROC: 0.9805, Recall: 0.9895\n",
            "Validation - Accuracy: 0.8554, AUC-ROC: 0.8192, Recall: 0.9176\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 459/459 [00:21<00:00, 20.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - Loss: 0.0207, Accuracy: 0.9943, AUC-ROC: 0.9932, Recall: 0.9964\n",
            "Validation - Accuracy: 0.8529, AUC-ROC: 0.8154, Recall: 0.9176\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 459/459 [00:21<00:00, 20.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Loss: 0.0197, Accuracy: 0.9935, AUC-ROC: 0.9925, Recall: 0.9951\n",
            "Validation - Accuracy: 0.8358, AUC-ROC: 0.8007, Recall: 0.8961\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 459/459 [00:21<00:00, 20.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 - Loss: 0.0094, Accuracy: 0.9975, AUC-ROC: 0.9975, Recall: 0.9976\n",
            "Validation - Accuracy: 0.8456, AUC-ROC: 0.8100, Recall: 0.9068\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 459/459 [00:22<00:00, 20.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 - Loss: 0.0102, Accuracy: 0.9967, AUC-ROC: 0.9961, Recall: 0.9980\n",
            "Validation - Accuracy: 0.8554, AUC-ROC: 0.8192, Recall: 0.9176\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 459/459 [00:21<00:00, 20.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 - Loss: 0.0084, Accuracy: 0.9975, AUC-ROC: 0.9973, Recall: 0.9980\n",
            "Validation - Accuracy: 0.8382, AUC-ROC: 0.7838, Recall: 0.9319\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 459/459 [00:22<00:00, 20.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 - Loss: 0.0054, Accuracy: 0.9989, AUC-ROC: 0.9988, Recall: 0.9992\n",
            "Validation - Accuracy: 0.8382, AUC-ROC: 0.7879, Recall: 0.9247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=8)\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = {key: value.to(device) for key, value in batch.items()}\n",
        "    batch['labels'] = batch['label']\n",
        "    del batch['label']\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "    logits = outputs.logits\n",
        "    test_preds.append(logits.detach())\n",
        "    test_labels.append(batch['labels'].detach())\n",
        "\n",
        "test_preds = torch.cat(test_preds)\n",
        "test_labels = torch.cat(test_labels)\n",
        "\n",
        "test_acc = accuracy(test_preds, test_labels)\n",
        "test_auc = auc_roc(test_preds, test_labels)\n",
        "test_rec = recall(test_preds, test_labels)\n",
        "print(f\"Test - Accuracy: {test_acc:.4f}, AUC-ROC: {test_auc:.4f}, Recall: {test_rec:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C9xBMbu1nWR",
        "outputId": "1c5028e2-b0a5-47b0-f4d3-1d5238601359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test - Accuracy: 0.8377, AUC-ROC: 0.8028, Recall: 0.9085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0P50LjKK7dq2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}